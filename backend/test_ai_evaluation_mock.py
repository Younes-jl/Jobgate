#!/usr/bin/env python3
"""
Script de test pour l'√©valuation IA avec service mock
Simule l'√©valuation sans t√©l√©charger de vraies vid√©os
"""

import os
import django
import json
from datetime import datetime

# Configuration Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'prototype.settings')
django.setup()

from interviews.models import InterviewAnswer, AiEvaluation
from users.models import CustomUser


class MockAIVideoEvaluationService:
    """Service mock pour tester l'√©valuation IA sans vraies vid√©os"""
    
    def evaluate_interview_answer(self, answer_id: int) -> dict:
        """
        Simule une √©valuation IA compl√®te
        """
        try:
            # R√©cup√©rer la r√©ponse
            answer = InterviewAnswer.objects.get(id=answer_id)
            
            # Simuler une transcription
            mock_transcription = f"Bonjour, je suis ravi de r√©pondre √† cette question sur {answer.question.text[:30]}... Je pense que mon exp√©rience en d√©veloppement web me permet d'apporter une valeur ajout√©e √† votre √©quipe. J'ai travaill√© sur plusieurs projets utilisant React, Django et PostgreSQL."
            
            # Simuler une √©valuation IA
            mock_evaluation = {
                "communication_score": 8.5,
                "relevance_score": 7.8,
                "confidence_score": 8.2,
                "overall_score": 8.2,
                "strengths": "Excellente articulation, exemples concrets, bonne structure de r√©ponse",
                "weaknesses": "Pourrait d√©velopper davantage les aspects techniques, manque quelques d√©tails sur l'exp√©rience r√©cente",
                "feedback": "Candidat prometteur avec une bonne capacit√© de communication. La r√©ponse est bien structur√©e et montre une compr√©hension solide du domaine. Recommandation: approfondir les aspects techniques lors d'un prochain entretien."
            }
            
            # Cr√©er ou mettre √† jour l'√©valuation IA
            ai_evaluation, created = AiEvaluation.objects.update_or_create(
                interview_answer=answer,
                defaults={
                    'transcription': mock_transcription,
                    'communication_score': mock_evaluation['communication_score'],
                    'relevance_score': mock_evaluation['relevance_score'],
                    'confidence_score': mock_evaluation['confidence_score'],
                    'overall_ai_score': mock_evaluation['overall_score'],
                    'strengths': mock_evaluation['strengths'],
                    'weaknesses': mock_evaluation['weaknesses'],
                    'ai_feedback': mock_evaluation['feedback'],
                    'status': 'completed',
                    'processing_time': 15.5,  # Simuler 15.5 secondes
                    'ai_provider': 'gemini'
                }
            )
            
            print(f"‚úÖ √âvaluation IA {'cr√©√©e' if created else 'mise √† jour'} pour la r√©ponse {answer_id}")
            print(f"   Score global: {mock_evaluation['overall_score']}/10")
            print(f"   Communication: {mock_evaluation['communication_score']}/10")
            print(f"   Pertinence: {mock_evaluation['relevance_score']}/10")
            print(f"   Confiance: {mock_evaluation['confidence_score']}/10")
            
            return {
                'success': True,
                'evaluation_id': ai_evaluation.id,
                'scores': mock_evaluation,
                'transcription': mock_transcription
            }
            
        except InterviewAnswer.DoesNotExist:
            raise ValueError(f"R√©ponse d'entretien {answer_id} non trouv√©e")
        except Exception as e:
            raise Exception(f"Erreur lors de l'√©valuation mock: {str(e)}")


def main():
    """Test du service mock d'√©valuation IA"""
    print("üß™ Test du service mock d'√©valuation IA")
    print("=" * 50)
    
    # R√©cup√©rer les r√©ponses de test
    answers = InterviewAnswer.objects.filter(
        cloudinary_url__isnull=False
    ).order_by('id')
    
    if not answers.exists():
        print("‚ùå Aucune r√©ponse avec URL Cloudinary trouv√©e")
        return
    
    print(f"üìä {answers.count()} r√©ponses trouv√©es pour le test")
    
    # Initialiser le service mock
    service = MockAIVideoEvaluationService()
    
    # Tester chaque r√©ponse
    for answer in answers[:3]:  # Limiter √† 3 pour le test
        print(f"\nüéØ Test de la r√©ponse {answer.id}")
        print(f"   Question: {answer.question.text[:60]}...")
        print(f"   Candidat: {answer.candidate.username}")
        
        try:
            result = service.evaluate_interview_answer(answer.id)
            print(f"   ‚úÖ √âvaluation r√©ussie - ID: {result['evaluation_id']}")
            
        except Exception as e:
            print(f"   ‚ùå Erreur: {str(e)}")
    
    # Afficher le r√©sum√©
    print(f"\nüìà R√©sum√© des √©valuations:")
    evaluations = AiEvaluation.objects.filter(status='completed')
    print(f"   Total √©valuations: {evaluations.count()}")
    
    if evaluations.exists():
        from django.db.models import Avg
        avg_score = evaluations.aggregate(
            avg_score=Avg('overall_ai_score')
        )['avg_score']
        print(f"   Score moyen: {avg_score:.1f}/10")
    
    print("\nüéâ Test termin√© avec succ√®s!")


if __name__ == "__main__":
    main()
